name: CI/CD Pipeline for Staging

permissions:
  security-events: write
  actions: read
  contents: read
  packages: write

on:
  push:
    branches: [staging]
  pull_request:
    branches: [staging]

env:
  REGISTRY: ghcr.io

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '22'
          cache: 'npm'
          cache-dependency-path: 'frontend/package-lock.json'

      - name: Set up Java
        uses: actions/setup-java@v4
        with:
          java-version: '21'
          distribution: 'temurin'

      - name: Setup Gradle
        uses: gradle/actions/setup-gradle@v3
        with:
          gradle-version: wrapper

      - name: Install frontend dependencies
        working-directory: ./frontend
        run: npm ci

      - name: Run frontend linting
        working-directory: ./frontend
        run: npm run lint

      - name: Run frontend unit tests
        working-directory: ./frontend
        run: npm run test:unit

      - name: Build frontend
        working-directory: ./frontend
        run: npm run build

      - name: Run backend tests
        run: ./gradlew test

      - name: Run backend integration tests
        run: ./gradlew integrationTest || true

      - name: Generate test reports
        if: always()
        run: |
          echo "Frontend tests completed"
          echo "Backend tests completed"

  security-scan:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'

      - name: Upload Trivy scan results
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: 'trivy-results.sarif'

  build-and-push:
    needs: [test, security-scan]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/staging'
    outputs:
      image: ${{ steps.image.outputs.image }}
      tailscale_ip: ${{ steps.get-tailscale-ip.outputs.tailscale_ip }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Set up image name
        run: |
          echo "IMAGE_NAME=$(echo ${{ github.repository }} | tr '[:upper:]' '[:lower:]')" >> $GITHUB_ENV

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push Docker image
        id: build
        uses: docker/build-push-action@v5
        with:
          context: .
          platforms: linux/amd64
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Output image
        id: image
        run: |
          IMAGE_NAME_LOWER=$(echo ${{ github.repository }} | tr '[:upper:]' '[:lower:]')
          echo "image=${{ env.REGISTRY }}/${IMAGE_NAME_LOWER}:${{ github.ref_name }}" >> $GITHUB_OUTPUT

  deploy-staging:
    needs: build-and-push
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/staging'
    environment: staging
    steps:
      # Connect GitHub runner to Tailscale network
      - name: Connect to Tailscale
        uses: tailscale/github-action@v3
        with:
          oauth-client-id: ${{ secrets.TS_OAUTH_CLIENT_ID }}
          oauth-secret: ${{ secrets.TS_OAUTH_SECRET }}
          tags: tag:ci
          version: 1.76.1

      # Get the Tailscale IP of your staging server
      - name: Get staging server Tailscale IP
        id: staging-ip
        run: |
          # Replace 'abet-app-server' with your actual server hostname in Tailscale
          # Or use the IP directly if you know it
          STAGING_IP=$(tailscale ip -4 abet-app-server)
          echo "ip=$STAGING_IP" >> $GITHUB_OUTPUT
          echo "Staging server Tailscale IP: $STAGING_IP"

      # Configure SSH with direct commands
      - name: Configure SSH
        run: |
          mkdir -p ~/.ssh/
          echo "${{ secrets.DROPLET_SSH_KEY }}" > ~/.ssh/staging.key
          chmod 600 ~/.ssh/staging.key
          cat >>~/.ssh/config <<END
          Host staging
            HostName ${{ steps.staging-ip.outputs.ip }}
            User ${{ secrets.DROPLET_USER }}
            IdentityFile ~/.ssh/staging.key
            StrictHostKeyChecking no
          END

      # Deploy via Tailscale network (more secure than public SSH)
      - name: Deploy to staging via Tailscale
        run: |
          ssh staging << 'EOF'
            set -e
            cd /opt/app
          
            echo "Starting staging deployment via Tailscale..."
          
            # Get Tailscale IP for security verification
            TAILSCALE_IP=$(tailscale ip -4)
            PUBLIC_IP=$(curl -s ifconfig.me)
          
            echo "Network Info:"
            echo "   Public IP: $PUBLIC_IP"  
            echo "   Tailscale IP: $TAILSCALE_IP"
          
            # Verify Tailscale is running
            if ! systemctl is-active --quiet tailscaled; then
                echo "ERROR: Tailscale is not running"
                exit 1
            fi
          
            # Verify firewall is active
            if ! ufw status | grep -q "Status: active"; then
                echo "ERROR: Firewall is not active"
                exit 1
            fi
          
            # Pull latest image
            echo "Pulling image: ${{ needs.build-and-push.outputs.image }}"
            docker pull ${{ needs.build-and-push.outputs.image }}
          
            # Update Docker Compose with Tailscale IP
            sed -i "s/100\.[0-9]*\.[0-9]*\.[0-9]*:8081:8080/${TAILSCALE_IP}:8081:8080/g" docker-compose.staging.yml
          
            # Stop existing container
            echo "Stopping existing staging container..."
            docker-compose -f docker-compose.staging.yml down || true
          
            # Start new container
            echo "Starting new staging container..."
            IMAGE_TAG=${{ needs.build-and-push.outputs.image }} docker-compose -f docker-compose.staging.yml up -d
          
            # Wait for container to be ready
            echo "Waiting for application to start..."
            sleep 30
          
            echo "Staging deployment completed successfully!"
          EOF

      # Health check via Tailscale network
      - name: Health check via Tailscale
        run: |
          STAGING_IP="${{ steps.staging-ip.outputs.ip }}"
          echo "Running health check on $STAGING_IP:8081"
          
          # Wait for service to be ready
          for i in {1..30}; do
            if curl -f --max-time 10 "http://$STAGING_IP:8081/actuator/health"; then
              echo "✅ Health check passed!"
              break
            fi
            echo "Attempt $i failed, waiting 10 seconds..."
            sleep 10
          done

      # Security verification - ensure staging is only accessible via Tailscale
      - name: Verify Tailscale-only access
        run: |
          STAGING_IP="${{ steps.staging-ip.outputs.ip }}"
          
          # This should work (via Tailscale)
          if curl -f --max-time 10 "http://$STAGING_IP:8081/actuator/health"; then
            echo "✅ Tailscale access confirmed"
          else
            echo "❌ Cannot access via Tailscale"
            exit 1
          fi
          
          echo "✅ Security verified: Service accessible via Tailscale"

      # Cleanup SSH configuration
      - name: Cleanup SSH
        if: always()
        run: |
          rm -f ~/.ssh/staging.key
          rm -f ~/.ssh/config

  # Enhanced E2E tests with Tailscale access
  e2e-testing:
    needs: deploy-staging
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/staging'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      # Connect to Tailscale for E2E testing
      - name: Connect to Tailscale for testing
        uses: tailscale/github-action@v3
        with:
          oauth-client-id: ${{ secrets.TS_OAUTH_CLIENT_ID }}
          oauth-secret: ${{ secrets.TS_OAUTH_SECRET }}
          tags: tag:ci

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '22'
          cache: 'npm'
          cache-dependency-path: 'frontend/package-lock.json'

      - name: Install dependencies
        working-directory: ./frontend
        run: npm ci

      - name: Install Playwright browsers
        working-directory: ./frontend
        run: npx playwright install --with-deps

      # Get staging server IP via Tailscale
      - name: Get staging server IP
        id: staging-ip
        run: |
          STAGING_IP=$(tailscale ip -4 abet-app-server)
          echo "ip=$STAGING_IP" >> $GITHUB_OUTPUT
          echo "Staging IP: $STAGING_IP"

      - name: Run E2E tests against staging
        working-directory: ./frontend
        env:
          # Now E2E tests can access staging via Tailscale
          PLAYWRIGHT_BASE_URL: http://${{ steps.staging-ip.outputs.ip }}:8081
        run: |
          echo "Running E2E tests against: $PLAYWRIGHT_BASE_URL"
          npx playwright test --config=playwright.config.ts

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: playwright-report
          path: frontend/playwright-report/

  notify-staging-ready:
    needs: deploy-staging
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/staging'
    steps:
      - name: Notify staging deployment
        run: |
          echo "Staging deployment completed!"
          echo "Access URL: http://${{ needs.deploy-staging.outputs.tailscale_ip }}:8081"
          echo "Only accessible via Tailscale network"
          echo "Manual testing required"

  deploy-production:
    needs: [deploy-staging]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    environment: production
    steps:
      - name: Deploy to production via SSH
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.PRODUCTION_HOST }}
          username: ${{ secrets.PRODUCTION_USER }}
          key: ${{ secrets.PRODUCTION_SSH_KEY }}
          port: ${{ secrets.PRODUCTION_SSH_PORT || 22 }}
          script: |
            set -e
            cd /opt/app
            
            echo "Starting production deployment..."
            
            # Create docker-compose override for production (public access)
            cat > docker-compose.production.yml << EOF
            services:
              app:
                image: ${{ needs.build-and-push.outputs.image }}
                environment:
                  SPRING_PROFILES_ACTIVE: production
                ports:
                  - "80:8080"  # Public access
                restart: unless-stopped
                networks:
                  - app-network
                logging:
                  driver: "json-file"
                  options:
                    max-size: "10m"
                    max-file: "3"
            
            networks:
              app-network:
                driver: bridge
            EOF
            
            # Pull latest image
            echo "Pulling image: ${{ needs.build-and-push.outputs.image }}"
            docker pull ${{ needs.build-and-push.outputs.image }}
            
            # Blue-green deployment
            echo "Starting new production container..."
            docker-compose -f docker-compose.production.yml up -d --force-recreate
            
            # Health check
            echo "Running production health check..."
            for i in {1..15}; do
              if curl -f --max-time 10 http://localhost:80/actuator/health; then
                echo "Production health check passed"
                break
              else
                echo "Health check attempt $i failed, retrying..."
                sleep 10
                if [ $i -eq 15 ]; then
                  echo "Production deployment failed - rolling back"
                  docker-compose -f docker-compose.production.yml down
                  exit 1
                fi
              fi
            done
            
            # Clean up old images
            docker image prune -af --filter "until=72h"
            
            echo "Production deployment completed successfully!"
            echo "Public access: http://${{ secrets.DROPLET_IP }}"